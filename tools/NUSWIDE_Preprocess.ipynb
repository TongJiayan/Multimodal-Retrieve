{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import scipy.io as io\n",
    "import shutil\n",
    "\n",
    "def get_selected_samples_idx(labels_path):\n",
    "    # load AllLabels/*.txt \n",
    "    filenames = os.listdir(labels_path)\n",
    "    filenames.sort()\n",
    "    all_labels = []\n",
    "    for filename in filenames:\n",
    "        file_path = os.path.join(labels_path,filename)\n",
    "        all_labels.append(np.loadtxt(file_path))\n",
    "    all_labels = np.transpose(all_labels)\n",
    "    \n",
    "    # Extract 21 concepts which contains the most images\n",
    "    # Like many other authers did \"Hashing with Graph\",\"Learning Consistence..\"\n",
    "    # sizeOfEachLabel = all_labels.sum(axis = 0)\n",
    "    # labels21_idx = np.argsort(-sizeOfEachLabel)[:21]\n",
    "    # labels21 = all_labels[:,labels21_idx]\n",
    "    \n",
    "    # select single-labeled samples and return idx array\n",
    "    labelCountOfEachImage = all_labels.sum(axis = 1)\n",
    "    singleLabeledIdx = np.where(labelCountOfEachImage == 1)\n",
    "    selectedSamplesIdx = np.array(singleLabeledIdx)[0]\n",
    "    selectedSamplesIdx = selectedSamplesIdx.tolist()\n",
    "    \n",
    "    return selectedSamplesIdx, all_labels\n",
    "\n",
    "def split_trainset_and_testset_idx(selectedSampleIdx,trainset_ratio, savepath):\n",
    "    selectedSampleSize = np.size(selectedSampleIdx,0)\n",
    "    trainsetSize = int(selectedSampleSize * trainset_ratio)\n",
    "    trainSampleIdx = random.sample(selectedSampleIdx,trainsetSize)\n",
    "    testSampleIdx = np.setdiff1d(selectedSampleIdx,trainSampleIdx)\n",
    "    np.savetxt(os.path.join(savepath,'trainSampleIdx.txt'),trainSampleIdx)\n",
    "    np.savetxt(os.path.join(savepath,'testSampleIdx.txt'),testSampleIdx)\n",
    "    return trainSampleIdx,testSampleIdx\n",
    "\n",
    "def prepare_image_feature_mat(trainSampleIdx,testSampleIdx,CM55Path,savepath):\n",
    "    # Load images feature\n",
    "    allImageFeature = np.loadtxt(CM55Path)\n",
    "\n",
    "    # Save image features of trainset and testset \n",
    "    testImageFeature = allImageFeature[testSampleIdx,:]\n",
    "    trainImageFeature = allImageFeature[trainSampleIdx,:]\n",
    "    io.savemat(os.path.join(savepath,'nus_img_raw.mat'),{'raw_img_test':testImageFeature,\n",
    "                                                         'raw_img_train':trainImageFeature})\n",
    "\n",
    "def prepare_testset_groundtruth(testSampleIdx,allLabels,savepath):\n",
    "    testsetGroundTruthMatrix = np.mat(allLabels[testSampleIdx,:])\n",
    "    groundTruth = testsetGroundTruthMatrix.nonzero()[1]+1  \n",
    "    io.savemat(os.path.join(savepath,'nus_test_groundtruth.mat'),{'ground_truth':groundTruth})\n",
    "    return groundTruth\n",
    "    \n",
    "def prepare_text_feature_mat(trainSampleIdx,testSampleIdx,tags1kPath,savepath):\n",
    "    allTextFeature = np.loadtxt(tags1kPath)\n",
    "    trainTextFeature = allTextFeature[trainSampleIdx,:]\n",
    "    testTextFeature = allTextFeature[testSampleIdx,:]\n",
    "    io.savemat(os.path.join(savepath,'nus_text.mat'),{'text_test':testTextFeature,\n",
    "                                                         'text_train':trainTextFeature})\n",
    "\n",
    "def prepare_raw_image_directory(trainSampleIdx,testSampleIdx,imagelistPath,imageSourcePath,imageTargetPath):\n",
    "    imageListFile = open(imagelistPath)\n",
    "    allImages = []\n",
    "    trainImages = []\n",
    "    testImages = []\n",
    "    for image in imageListFile.readlines():\n",
    "        allImages.append(image)   \n",
    "    for idx in trainSampleIdx:\n",
    "        trainImages.append(allImages[idx])\n",
    "    for idx in testSampleIdx:\n",
    "        testImages.append(allImages[idx])\n",
    "\n",
    "    for idx, filename in enumerate(trainImages):\n",
    "        sourceFilePath = os.path.join(imageSourcePath,filename.strip()).replace('\\\\','/')\n",
    "        newFilename = idx_filename_prefix(idx) + str(idx) + '.jpg'\n",
    "        targetFilePath = os.path.join(imageTargetPath,'trainset',newFilename)\n",
    "        shutil.copyfile(sourceFilePath,targetFilePath)\n",
    "    for idx, filename in enumerate(testImages):\n",
    "        sourceFilePath = os.path.join(imageSourcePath,filename.strip()).replace('\\\\','/')\n",
    "        newFilename = idx_filename_prefix(idx) + str(idx) + '.jpg'\n",
    "        targetFilePath = os.path.join(imageTargetPath,'testset',newFilename)\n",
    "        shutil.copyfile(sourceFilePath,targetFilePath)\n",
    "        \n",
    "def idx_filename_prefix(idx):\n",
    "    if idx < 10:\n",
    "        return '00000'\n",
    "    elif idx < 100:\n",
    "        return '0000'\n",
    "    elif idx < 1000:\n",
    "        return '000'\n",
    "    elif idx < 10000:\n",
    "        return '00'\n",
    "    elif idx < 100000:\n",
    "        return '0'\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 79216 samples for experiment\n",
    "selectedSampleIdx,allLabels = get_selected_samples_idx('raw_image_nuswide/AllLabels/')\n",
    "# Split selected samples to trainset(52810) and testset(26406)\n",
    "trainSampleIdx, testSampleIdx = split_trainset_and_testset_idx(selectedSampleIdx, 2/3,'raw_image_nuswide')\n",
    "prepare_image_feature_mat(trainSampleIdx,testSampleIdx,'raw_image_nuswide/Normalized_CM55.txt','raw_image_nuswide/')\n",
    "prepare_testset_groundtruth(testSampleIdx,allLabels,'raw_image_nuswide')\n",
    "prepare_text_feature_mat(trainSampleIdx,testSampleIdx,'raw_image_nuswide/AllTags1k.txt','raw_image_nuswide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_raw_image_directory(trainSampleIdx,testSampleIdx,'raw_image_nuswide/Imagelist.txt','raw_image_nuswide/NUSWIDE_IMAGES/Flickr','raw_image_nuswide/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_image_nuswide/NUSWIDE_IMAGES/Flickr/sss\n"
     ]
    }
   ],
   "source": [
    "a = 'raw_image_nuswide/NUSWIDE_IMAGES/Flickr\\\\sss'\n",
    "b = a.replace('\\\\','/')\n",
    "print(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
